name: Linux.Triage.UAC
description: |
  NOTE:
    This artifact was built from [The Velociraptor Triage
    Repository](https://triage.velocidex.com/docs/)

  Commit {{ .Commit }} on {{ .Time }}

parameters:
  - name: MaxFileSize
    type: int
    default: 18446744073709551615
    description: |
      The max size in bytes of the individual files to collect.
      Set to 0 to disable it.

  - name: UPLOAD_IS_RESUMABLE
    type: bool
    default: Y
    description: |
      If set the uploads can be resumed if the flow times out or
      errors.
{{ range .TargetFiles }}
  - name: {{ .Name }}
    description: "{{ .Description }}"
    type: bool
{{ end }}

export: |
  LET VQL_MATERIALIZE_ROW_LIMIT <= 10000
  LET S = scope()

  -- Group the targets for faster searching.
  LET TargetTable <= SELECT Target,
       enumerate(items=dict(Rule=Rule, Glob=Glob, Ref=Ref)) AS Rules
    FROM parse_csv(accessor="data",
  filename='''
  Target,Rule,Glob,Ref
  {{- range .Rules }}
  {{ .Target}},{{ .Name }},"{{ .Glob }}",{{ .Ref -}}
  {{ end }}
  ''')
  GROUP BY Target

  //  Build a lookup cache on target.
  LET Lookup <= memoize(query={
    SELECT * FROM TargetTable
  }, key="Target")

  -- Extract all rules within the required target. Uses the memoized
  -- structure above.
  LET FilterTable(Required) =
     SELECT Required AS Target, *
     FROM flatten(query={
       SELECT * FROM foreach(row=get(item=Lookup, field=Required).Rules)
     })
     WHERE if(condition=Glob =~ SlowGlobRegex,
              then=log(message="Dropping rule %v/%v because it is too slow: %v",
                       dedup=-1, args=[Target, Rule, Glob]) AND FALSE,
              else=TRUE)

  LET Expand(FilteredTable) = SELECT * FROM foreach(
  row=FilteredTable,
  query={
    -- If there is a reference, resolve it from the table recursively.
    SELECT *
    FROM if(condition=Ref AND log(message="%v/%v: Resolving Ref %v", dedup=-1, args=[Target, Rule, Ref]),
    then={
       SELECT * FROM Expand(
          FilteredTable={
             SELECT * FROM FilterTable(Required=Ref)
          })
    }, else={
       SELECT Target, Rule, Glob FROM scope()
    })
  })

sources:
- name: SearchGlobs
  query: |
    -- Collect all the top level targets that the user selected.
    LET Collections <= SELECT Target + "/" + Rule AS Rule, Glob
    FROM Expand(FilteredTable={
      SELECT Target,
            Rules.Rule AS Rule,
            Rules.Glob AS Glob,
            Rules.Ref AS Ref
     FROM flatten(query={
       SELECT * FROM TargetTable
       WHERE get(field=Target)
        AND log(message="Collecting target %v: %v", args=[Target, Rule], dedup=-1)
     })
    })
    GROUP BY Rule, Glob

    SELECT * FROM Collections

- name: All Matches Metadata
  query: |
    LET GlobLookup <= memoize(query=Collections, key="Glob")
    LET _ <= if(condition=MaxFileSize > 0,
                then=log(message="Limiting file acquisition to MaxFileSize %v bytes (%v)",
                         args=[MaxFileSize, humanize(bytes=MaxFileSize)]))

    LET AllResults <= SELECT OSPath AS SourceFile,
                             Size,
                             Btime AS Created,
                             Ctime AS Changed,
                             Mtime AS Modified,
                             Atime AS LastAccessed,
                             Accessor
    FROM foreach(row={
        SELECT _value AS Device FROM foreach(row=Devices)
    }, query={
      SELECT * FROM chain(async=TRUE,
      a={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               "ntfs" AS Accessor
        FROM glob(globs=NTFSGlobs.Glob, accessor="ntfs", root=Device)
      }, b={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule
        FROM glob(globs=AutoGlobs.Glob,
                  accessor="auto")
      })
    })
    WHERE NOT IsDir
    AND log(message="Found %v for rule %v", args=[SourceFile, Rule], dedup=10)
    AND if(condition= Size <= MaxFileSize,
           then=TRUE,
           else=log(message="Skipping file %v (Size %v) Due to MaxFileSize",
                    dedup=-1, args=[SourceFile, humanize(bytes=Size)]) AND FALSE)

    SELECT * FROM AllResults

- name: Uploads
  query: |
    -- Upload the files. Split into workers so the files are uploaded in parallel.
    LET uploaded_files = SELECT *
    FROM foreach(row={
       SELECT * FROM AllResults
       },
          workers=30,
          query={
            SELECT now() AS CopiedOnTimestamp,
                   Created,
                   Changed,
                   LastAccessed,
                   Modified,
                   SourceFile,
                   Size,
                   upload(file=SourceFile, accessor=Accessor, mtime=Modified) AS Upload
            FROM scope()
      })

    -- Separate the hashes into their own column.
    SELECT CopiedOnTimestamp,
           SourceFile,
           Upload.Path AS DestinationFile,
           Size AS FileSize,
           Upload.sha256 AS SourceFileSha256,
           Created,
           Changed,
           Modified,
           LastAccessed
    FROM uploaded_files
