{{ define "GlobTable" }}
  Target,Rule,Glob,Ref
  {{- range .Rules }}
  {{ .Target}},{{ .Name }},"{{ .Glob }}",{{ .Ref -}}
  {{ end }}
{{ end -}}

name: {{ .Config.Name }}
description: |
  A backwards compatibility stub for Windows.Triage.Targets

  NOTE: This artifact is Deprecated and had been replaced by
  Windows.Triage.Targets. See that for detailed description.

  Commit {{ .Commit }} on {{ .Time }}

parameters:
  - name: HighLevelTargets
    description: A shorter list of Meta-Targets
    type: multichoice
    default: "[]"
    choices:
{{- range .TargetFiles }}
    {{- if hasPrefix "_" .Name }}
    - {{ .Name -}}
    {{ end -}}
{{ end }}

  - name: Targets
    type: multichoice
    description: All targets available
    default: "[]"
    choices:
{{- range .TargetFiles }}
    - {{ .Name -}}
{{ end }}

  - name: Devices
    type: json_array
    description: |
      Name of the drive letter to search. You can add multiple drives
      separated with a comma.
    default: '["C:"]'

  - name: CollectionPolicy
    type: choices
    choices:
      - ExcludeSigned
      - HashOnly
      - AllFiles
    default: ExcludeSigned

  - name: DropVerySlowRules
    type: bool
    default: Y
    description: Ignore inefficient targets

  - name: TrustedPathRegex
    type: regex
    default: ^C:\\\\Windows\\\\
    description: |
      Do not hash or upload any adaptive files matching this regex.

  - name: MaxFileSize
    type: int
    description: |
      The max size in bytes of the individual files to collect
      (Default 100Mb).

  - name: MaxHashSize
    type: int
    default: "100000000"
    description: |
      The max size in bytes of the individual files to hash.

  - name: UPLOAD_IS_RESUMABLE
    type: bool
    default: Y
    description: |
      If set the uploads can be resumed if the flow times out or
      errors.

  - name: VSS_MAX_AGE_DAYS
    type: int
    default: 0
    description: |
      If larger than zero we analyze VSS within this many days
      ago. (e.g 7 will analyze all VSS within the last week).  Note
      that when using VSS analysis we have to use the ntfs accessor
      for everything which will be much slower.

  - name: Verbose
    type: bool
    description: |
      Add Verbose debugging logs

imports:
  - Windows.Triage.Targets

sources:
- name: SearchGlobs
  notebook:
    - type: none
  query: |
{{ if .Dependencies }}
    // Work around bug in the artifact compiler failing to detect deps in export section
{{ range $k, $v := .Dependencies }}
    LET _ = SELECT * FROM Artifact.{{ $k }}()
{{ end }}
{{ end }}
    LET Targets <= to_dict(item={
      SELECT _value AS _key, TRUE AS _value
      FROM foreach(row=Targets + HighLevelTargets)
    })

    LET AllCollections <= Collections(Targets=Targets)

    SELECT * FROM AllCollections
    --WHERE Glob

- name: All Matches Metadata
  notebook:
    - type: vql
      template: |
       // This cell generates other cells to preview the collected
       // data.  DO NOT recalculate this cell - each time new cells
       // will be added. Instead delete the notebook and allow
       // Velociraptor to recreate the entire notebook.
       LET ArtifactsWithResults <=
         SELECT pathspec(accessor="fs", parse=Data.VFSPath)[4] AS Artifact ,
           pathspec(accessor="fs", parse=Data.VFSPath)[-1][:-5] AS Source ,
           stat(accessor="fs", filename=Data.VFSPath + ".index").Size / 8 AS Records
         FROM enumerate_flow(client_id=ClientId, flow_id=FlowId)
         WHERE Type =~ "Result" AND Records > 0

       LET _ <= SELECT notebook_update_cell(notebook_id=NotebookId, type="vql",
       input=format(format='''
       /*
       # Results From %v
       */
       SELECT * FROM source(source=%q)
       ''', args=[Source, Source]),
       output=format(format='''
       <i>Recalculate</i> to show Results from <b>%v</b> with <b>%v</b> rows
       ''', args=[Source, Records])) AS NotebookModification
       FROM ArtifactsWithResults

       /*
       # Results Overview
       */
       SELECT Source, Records FROM ArtifactsWithResults ORDER BY Source


  query: |
    LET GlobLookup <= memoize(query=AllCollections, key="Glob")
    LET NTFSGlobs = SELECT * FROM AllCollections
       WHERE Glob AND Glob =~ "[:$]" AND NOT Glob =~ "\\$Recycle.Bin"
    LET AutoGlobs = SELECT * FROM AllCollections
       WHERE Glob AND ( Glob =~ "\\$Recycle.Bin" OR NOT Glob =~ "[:$]" )

    LET _ <= MaxFileSize > 0 && MaybeLOG(
      Message="Limiting file acquisition to MaxFileSize %v bytes (%v)",
      Args=[MaxFileSize, humanize(bytes=MaxFileSize)])

    LET PreferredAccessor <= if(
       condition=VSS_MAX_AGE_DAYS > 0,
       then="ntfs_vss", else="auto")

    LET AllGlobs = SELECT *
    FROM foreach(row={
        SELECT _value AS Device FROM foreach(row=Devices)
    }, query={
      SELECT * FROM chain(
      GlobNTFS={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               "ntfs" AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=NTFSGlobs.Glob, accessor="ntfs", root=Device)
      },
      GlobAuto={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               PreferredAccessor AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=AutoGlobs.Glob,
                  accessor=PreferredAccessor,
                  root=Device)
      })
    })
    WHERE NOT IsDir

    LET AllResults <= SELECT Type,
                             OSPath AS SourceFile,
                             Size,
                             Btime AS Created,
                             Ctime AS Changed,
                             Mtime AS Modified,
                             Atime AS LastAccessed,
                             Accessor,
                             Data
    FROM chain(
      {{ range $idx, $r := .Rules -}}
      {{- if $r.VQL -}}
      {{ $r.Target }}_{{ $r.Name }}={
        SELECT * FROM if(condition={
          SELECT * FROM AllCollections
          WHERE Rule = "{{ $r.Target }}/{{ $r.Name }}"
            AND MaybeLOG(
              Message="Collecting Custom VQL Rule %s",
              Args="{{ $r.Target }}/{{ $r.Name }}")
        }, then={
          SELECT "{{ $r.Target }}/{{ $r.Name }}" AS Type,
                 "{{ $r.Target }}/{{ $r.Name }}" AS Rule,
                 *
          FROM Collect{{ $r.Target }}_{{ $r.Name }}
        })
      },
      {{- end -}}
      {{- end }}
      Globs=AllGlobs
    )
    WHERE log(level="INFO", message="Found %v for rule %v", args=[
              SourceFile, Rule], dedup=10)

    SELECT * FROM AllResults
{{ range $idx, $r := .Rules -}}
{{- if $r.VQL }}
- name: {{ $r.Target }}_{{ $r.Name }}
  notebook:
    - type: none
  query: |
    SELECT * FROM foreach(row={
      SELECT dict(SourceFile=SourceFile, Size=Size, Modified=Modified) + Data AS Data
      FROM AllResults
      WHERE Type = "{{ $r.Target }}/{{ $r.Name }}"
    }, column="Data")
{{- end -}}
{{- end }}

- name: Uploads
  query: |
    // Initialize libmagic before we call it from multiple threads.
    LET _ <= magic(path="", accessor="data")

    -- Upload the files. Split into workers so the files are uploaded
    -- in parallel.
    LET uploaded_files = SELECT *
    FROM foreach(row={
       SELECT *
       FROM AllResults
       WHERE Size > 0
       GROUP BY SourceFile
       },
          workers=30,

          // Do the heavy lifting in a thread
          query={
            SELECT * FROM foreach(row={
              SELECT GetDetails(OSPath=SourceFile) AS Details
              FROM scope()
            }, query={
              SELECT timestamp(epoch=now()) AS CopiedOnTimestamp,
                     Created,
                     Changed,
                     LastAccessed,
                     Modified,
                     SourceFile,
                     Size,
                     Details,
                     if(condition=Details.ShouldUpload,
                        then=upload(file=SourceFile,
                                    accessor=Accessor,
                                    mtime=Modified)) AS Upload
              FROM scope()
          })
      })

    -- Separate the hashes into their own column.
    SELECT CopiedOnTimestamp,
           SourceFile,
           Upload.Path AS DestinationFile,
           Size AS FileSize,
           Details.Hash.SHA256 AS SourceFileSha256,
           Created,
           Changed,
           Modified,
           LastAccessed,
           Details,
           Upload
    FROM uploaded_files

  notebook:
    - type: none
    - type: vql_suggestion
      name: Post process collection
      template: |
        /*

        # Post process this collection.

        Uncomment the following and evaluate the cell to create new
        collections based on the files collected from this artifact.

        The below VQL will apply remapping so standard artifacts will
        see the KapeFiles.Targets collection below as a virtual
        Windows Client. The artifacts will be collected to a temporary
        container and then re-imported as new collections into this
        client.

        NOTE: This is only a stop gap in case the proper artifacts
        were not collected in the first place. Parsing artifacts
        through a remapped collection is not as accurate as parsing
        directly on the endpoint. See
        https://docs.velociraptor.app/training/playbooks/preservation/
        for more info.

        */
        LET _ <= import(artifact="Windows.KapeFiles.Remapping")

        LET tmp <= tempfile()

        LET Results = SELECT import_collection(filename=Container, client_id=ClientId) AS Import
        FROM collect(artifacts=[
                       "Windows.Forensics.Usn",
                       "Windows.NTFS.MFT",
                     ],
                     args=dict(`Windows.Forensics.Usn`=dict(),
                               `Windows.NTFS.MFT`=dict()),
                     output=tmp,
                     remapping=GetRemapping(FlowId=FlowId, ClientId=ClientId))

        // SELECT * FROM Results

column_types:
- name: CopiedOnTimestamp
  type: timestamp
- name: Data
  type: json/1
- name: Details
  type: json/1
