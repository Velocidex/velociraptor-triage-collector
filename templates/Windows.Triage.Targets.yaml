{{ define "GlobTable" }}
  Target,Rule,Glob,Ref
  {{- range .Rules }}
  {{ .Target}},{{ .Name }},"{{ .Glob }}",{{ .Ref -}}
  {{ end }}
{{ end -}}

name: {{ .Config.Name }}
description: |
  This artifact aims to preserve raw files from the endpoint by
  selectively acquiring the most important files for the investigation
  at hand. The artfiact collates multiple rules from sources such as
  the KapeFiles repository.

  Kape is a popular bulk collector tool for triaging a system
  quickly. While KAPE itself is not an opensource tool, the logic it
  uses to decide which files to collect is encoded in YAML files
  hosted on the KapeFiles project
  (https://github.com/EricZimmerman/KapeFiles) and released under an
  MIT license.

  This artifact is automatically generated from these YAML files,
  contributed and maintained by the community. This artifact only
  encapsulates the KAPE "Targets" - basically a bunch of glob
  expressions used for collecting files on the endpoint. We do not
  do any post processing of these files - we just collect them.

  We recommend that timeouts and upload limits be used
  conservatively with this artifact because we can upload really
  vast quantities of data very quickly.

  NOTE:
    This artifact was built from [The Velociraptor Triage
    Repository](https://triage.velocidex.com/docs/)

  Commit {{ .Commit }} on {{ .Time }}

reference:
  - https://www.kroll.com/en/insights/publications/cyber/kroll-artifact-parser-extractor-kape
  - https://github.com/EricZimmerman/KapeFiles

parameters:
  - name: Devices
    type: json_array
    description: |
      Name of the drive letter to search. You can add multiple drives
      separated with a comma.
    default: '["C:"]'

  - name: DropVerySlowRules
    type: bool
    default: Y
    description: |
      Some rules are very slow due to a recursive search at the higher
      levels (For example a glob such as `C:\**\*.ini` ). These rules
      cause the collection to be very slow as the entire filesystem
      must be searched.

      By default we drop these rules but you can enable them if you
      like. This will cause the collection to be a lot slower.

  - name: VSS_MAX_AGE_DAYS
    type: int
    default: 0
    description: |
      If larger than zero we analyze VSS within this many days
      ago. (e.g 7 will analyze all VSS within the last week).  Note
      that when using VSS analysis we have to use the ntfs accessor
      for everything which will be much slower.

  - name: MaxFileSize
    type: int
    default: 18446744073709551615
    description: |
      The max size in bytes of the individual files to collect.
      Set to 0 to disable it.

  - name: UPLOAD_IS_RESUMABLE
    type: bool
    default: Y
    description: |
      If set the uploads can be resumed if the flow times out or
      errors.
{{ range .TargetFiles }}
  - name: {{ .Name }}
    description: "{{ .Description }}"
    type: bool
{{ end }}

export: |
  LET VQL_MATERIALIZE_ROW_LIMIT <= 10000
  LET NTFS_CACHE_TIME <= 100000
  LET NTFS_DISABLE_FULL_PATH_RESOLUTION <= TRUE
  LET S = scope()

  {{- if .Config.Debug }}
  LET GlobTable <= '''{{ template "GlobTable" . }}
  '''
  {{ else }}
  LET GlobTable <= gunzip(string=base64decode(string="{{ Compress "GlobTable" . }}"))
  {{ end }}

  LET SlowGlobRegex <= if(condition=S.DropVerySlowRules,
     then="^\\*\\*", else="RunSlowFileGlobs!!!")

  -- Group the targets for faster searching.
  LET TargetTable <= SELECT Target,
       enumerate(items=dict(Rule=Rule, Glob=Glob, Ref=Ref)) AS Rules
    FROM parse_csv(accessor="data",
  filename=GlobTable)
  GROUP BY Target

  //  Build a lookup cache on target.
  LET Lookup <= memoize(query={
    SELECT * FROM TargetTable
  }, key="Target")

  -- Extract all rules within the required target. Uses the memoized
  -- structure above.
  LET FilterTable(Required) =
     SELECT Required AS Target, *
     FROM flatten(query={
       SELECT * FROM foreach(row=get(item=Lookup, field=Required).Rules)
     })
     WHERE if(condition=Glob =~ SlowGlobRegex,
              then=log(message="Dropping rule %v/%v because it is too slow: %v",
                       dedup=-1, args=[Target, Rule, Glob]) AND FALSE,
              else=TRUE)

  LET Expand(FilteredTable) = SELECT * FROM foreach(
  row=FilteredTable,
  query={
    -- If there is a reference, resolve it from the table recursively.
    SELECT *
    FROM if(condition=Ref AND log(message="%v/%v: Resolving Ref %v", dedup=-1, args=[Target, Rule, Ref]),
    then={
       SELECT * FROM Expand(
          FilteredTable={
             SELECT * FROM FilterTable(Required=Ref)
          })
    }, else={
       SELECT Target, Rule, Glob FROM scope()
    })
  })

  -- Collect all the top level targets that the user selected.
  LET Collections(Targets) = SELECT Target + "/" + Rule AS Rule, Glob
    FROM Expand(FilteredTable={
      SELECT Target,
            Rules.Rule AS Rule,
            Rules.Glob AS Glob,
            Rules.Ref AS Ref
     FROM flatten(query={
       SELECT * FROM TargetTable
       WHERE get(item=Targets, field=Target)
        AND log(message="Collecting target %v: %v", args=[Target, Rule], dedup=-1)
     })
    })
    GROUP BY Rule, Glob

    {{ range $idx, $r := .Rules -}}
    {{- if $r.VQL -}}
    // {{ .Description }}
    LET Collect{{ $r.Target }}_{{ $r.Name }} =
{{ Indent .VQL 7 -}}
    {{- end -}}
    {{- end }}

sources:
- name: SearchGlobs
  query: |
    LET AllCollections <= Collections(Targets=scope())

    SELECT * FROM AllCollections
    WHERE Glob

- name: All Matches Metadata
  query: |
    LET GlobLookup <= memoize(query=AllCollections, key="Glob")
    LET NTFSGlobs = SELECT * FROM AllCollections
       WHERE Glob AND Glob =~ "[:$]" AND NOT Glob =~ "\\$Recycle.Bin"
    LET AutoGlobs = SELECT * FROM AllCollections
       WHERE Glob AND ( Glob =~ "\\$Recycle.Bin" OR NOT Glob =~ "[:$]" )

    LET _ <= if(condition=MaxFileSize > 0,
                then=log(message="Limiting file acquisition to MaxFileSize %v bytes (%v)",
                         args=[MaxFileSize, humanize(bytes=MaxFileSize)]))

    LET PreferredAccessor <= if(
       condition=VSS_MAX_AGE_DAYS > 0,
       then="ntfs_vss", else="auto")

    LET AllResults <= SELECT OSPath AS SourceFile,
                             Size,
                             Btime AS Created,
                             Ctime AS Changed,
                             Mtime AS Modified,
                             Atime AS LastAccessed,
                             Accessor,
                             Data
    FROM foreach(row={
        SELECT _value AS Device FROM foreach(row=Devices)
    }, query={
      SELECT * FROM chain(async=TRUE,
      {{ range $idx, $r := .Rules -}}
      {{- if $r.VQL -}}
      {{ $r.Target }}_{{ $r.Name }}={
        SELECT * FROM if(condition={
          SELECT * FROM AllCollections
          WHERE Rule = "{{ $r.Target }}/{{ $r.Name }}"
        }, then=Collect{{ $r.Target }}_{{ $r.Name }})
      },
      {{- end -}}
      {{- end }}
      GlobNTFS={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               "ntfs" AS Accessor,
               dict(Globs=Globs) AS Data
        FROM glob(globs=NTFSGlobs.Glob, accessor="ntfs", root=Device)
      },
      GlobAuto={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               PreferredAccessor AS Accessor,
               dict(Globs=Globs) AS Data
        FROM glob(globs=AutoGlobs.Glob,
                  accessor=PreferredAccessor,
                  root=Device)
      })
    })
    WHERE NOT IsDir
    AND log(message="Found %v for rule %v", args=[SourceFile, Rule], dedup=10)
    AND if(condition= Size <= MaxFileSize,
           then=TRUE,
           else=log(message="Skipping file %v (Size %v) Due to MaxFileSize",
                    dedup=-1, args=[SourceFile, humanize(bytes=Size)]) AND FALSE)

    SELECT * FROM AllResults

- name: Uploads
  query: |
    -- Upload the files. Split into workers so the files are uploaded in parallel.
    LET uploaded_files = SELECT *
    FROM foreach(row={
       SELECT * FROM AllResults
       },
          workers=30,
          query={
            SELECT now() AS CopiedOnTimestamp,
                   Created,
                   Changed,
                   LastAccessed,
                   Modified,
                   SourceFile,
                   Size,
                   upload(file=SourceFile, accessor=Accessor, mtime=Modified) AS Upload
            FROM scope()
      })

    -- Separate the hashes into their own column.
    SELECT CopiedOnTimestamp,
           SourceFile,
           Upload.Path AS DestinationFile,
           Size AS FileSize,
           Upload.sha256 AS SourceFileSha256,
           Created,
           Changed,
           Modified,
           LastAccessed
    FROM uploaded_files
