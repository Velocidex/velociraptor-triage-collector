{{ define "GlobTable" }}
  Target,Rule,Glob,Ref
  {{- range .Rules }}
  {{ .Target}},{{ .Name }},"{{ .Glob }}",{{ .Ref -}}
  {{ end }}
{{ end -}}

name: {{ .Config.Name }}
description: |
  This artifact aims to preserve raw files from the endpoint by
  selectively acquiring the most important files for the investigation
  at hand. The artfiact collates multiple rules from sources such as
  the KapeFiles repository.

  Kape is a popular bulk collector tool for triaging a system
  quickly. While KAPE itself is not an opensource tool, the logic it
  uses to decide which files to collect is encoded in YAML files
  hosted on the KapeFiles project
  (https://github.com/EricZimmerman/KapeFiles) and released under an
  MIT license.

  This artifact is automatically generated from these YAML files,
  contributed and maintained by the community. This artifact only
  encapsulates the KAPE "Targets" - basically a bunch of glob
  expressions used for collecting files on the endpoint. We do not
  do any post processing of these files - we just collect them.

  We recommend that timeouts and upload limits be used
  conservatively with this artifact because we can upload really
  vast quantities of data very quickly.

  NOTE:
    This artifact was built from [The Velociraptor Triage
    Repository](https://triage.velocidex.com/docs/)

  ## CollectionPolicy

  The decision on whether to collect the file or simply display
  metadata (like hash etc) is determined by the CollectionPolicy:

  1. ExcludeSigned: If the file is a signed executable, we do not
     collect it.
  2. HashOnly: Do not collect any file - just display their hashes
  3. AllFiles: Collect all files regardless of their types.

  Commit {{ .Commit }} on {{ .Time }}

reference:
  - https://www.kroll.com/en/insights/publications/cyber/kroll-artifact-parser-extractor-kape
  - https://github.com/EricZimmerman/KapeFiles

parameters:
  - name: Devices
    type: json_array
    description: |
      Name of the drive letter to search. You can add multiple drives
      separated with a comma.
    default: '["C:"]'

  - name: CollectionPolicy
    type: choices
    choices:
      - ExcludeSigned
      - HashOnly
      - AllFiles
    default: ExcludeSigned

  - name: DropVerySlowRules
    type: bool
    default: Y
    description: |
      Some rules are very slow due to a recursive search at the higher
      levels (For example a glob such as `C:\**\*.ini` ). These rules
      cause the collection to be very slow as the entire filesystem
      must be searched.

      By default we drop these rules but you can enable them if you
      like. This will cause the collection to be a lot slower.

  - name: VSS_MAX_AGE_DAYS
    type: int
    default: 0
    description: |
      If larger than zero we analyze VSS within this many days
      ago. (e.g 7 will analyze all VSS within the last week).  Note
      that when using VSS analysis we have to use the ntfs accessor
      for everything which will be much slower.

  - name: MaxFileSize
    type: int
    description: |
      The max size in bytes of the individual files to collect (Default 0 means unlimited).

  - name: UPLOAD_IS_RESUMABLE
    type: bool
    default: Y
    description: |
      If set the uploads can be resumed if the flow times out or
      errors.
{{ range .TargetFiles }}
  - name: {{ .Name }}
    description: "{{ .Description }}"
    type: bool
{{ end }}

export: |
  LET VQL_MATERIALIZE_ROW_LIMIT <= 10000
  LET NTFS_CACHE_TIME <= 100000
  LET NTFS_DISABLE_FULL_PATH_RESOLUTION <= TRUE
  LET S = scope()

  LET CollectionPolicy <= S.CollectionPolicy || "ExcludeSigned"

  // Helper for VQL targets - try to download the file, but if failed,
  // we return an empty row to record the filename.
  LET TryToDownload(OSPath, Row) = SELECT * FROM switch(
  a={
    SELECT *, "auto" AS Accessor, Row AS Data
    FROM stat(filename=OSPath, accessor="auto")
  }, b={
    SELECT OSPath,
           0 AS Size,
           NULL AS Btime,
           NULL AS Ctime,
           NULL AS Mtime,
           NULL AS Atime,
           "" AS Accessor,
           Row AS Data
    FROM scope()
  })

  {{- if .Config.Debug }}
  LET GlobTable <= '''{{ template "GlobTable" . }}
  '''
  {{ else }}
  LET GlobTable <= gunzip(string=base64decode(string="{{ Compress "GlobTable" . }}"))
  {{ end }}

  LET SlowGlobRegex <= if(condition=S.DropVerySlowRules,
     then="^\\*\\*", else="RunSlowFileGlobs!!!")

  -- Group the targets for faster searching.
  LET TargetTable <= SELECT Target,
       enumerate(items=dict(Rule=Rule, Glob=Glob, Ref=Ref)) AS Rules
    FROM parse_csv(accessor="data",
  filename=GlobTable)
  GROUP BY Target

  //  Build a lookup cache on target.
  LET Lookup <= memoize(query={
    SELECT * FROM TargetTable
  }, key="Target")

  -- Extract all rules within the required target. Uses the memoized
  -- structure above.
  LET FilterTable(Required) =
     SELECT Required AS Target, *
     FROM flatten(query={
       SELECT * FROM foreach(row=get(item=Lookup, field=Required).Rules)
     })
     WHERE if(condition=Glob =~ SlowGlobRegex,
              then=log(level="INFO", message="Dropping rule %v/%v because it is too slow: %v",
                       dedup=-1, args=[Target, Rule, Glob]) AND FALSE,
              else=TRUE)

  LET Expand(FilteredTable) = SELECT * FROM foreach(
  row=FilteredTable,
  query={
    -- If there is a reference, resolve it from the table recursively.
    SELECT *
    FROM if(condition=Ref AND log(level="DEBUG", message="%v/%v: Resolving Ref %v",
         dedup=-1, args=[Target, Rule, Ref]),
    then={
       SELECT * FROM Expand(
          FilteredTable={
             SELECT * FROM FilterTable(Required=Ref)
          })
    }, else={
       SELECT Target, Rule, Glob FROM scope()
       WHERE if(condition=Glob,
         then=log(level="DEBUG", message="%v/%v: Glob is %v",
                  args=[Target, Rule, Glob], dedup=-1),
         else=TRUE)
    })
  })

  -- Collect all the top level targets that the user selected.
  LET Collections(Targets) = SELECT Target + "/" + Rule AS Rule, Glob
    FROM Expand(FilteredTable={
      SELECT Target,
            Rules.Rule AS Rule,
            Rules.Glob AS Glob,
            Rules.Ref AS Ref
     FROM flatten(query={
       SELECT * FROM TargetTable
       WHERE get(item=Targets, field=Target)
        AND log(level="INFO", message="Collecting target %v, Rules: %v",
             args=[Target, Rules.Rule], dedup=-1)
     })
    })
    GROUP BY Rule, Glob

    // In ExcludeSigned and HashOnly we dont upload signed binaries.
    LET ShouldUploadSignedBinary <= dict(
       ShouldUpload = NOT CollectionPolicy =~ "ExcludeSigned|HashOnly")

    // In HashOnly mode we never upload anything.
    LET ShouldUploadAnyFile <= dict(
       ShouldUpload = NOT CollectionPolicy =~ "HashOnly")

    LET DoNotUpload <= dict(ShouldUpload=FALSE)

    // Determine if we should upload the file based on signature.
    LET ShouldUpload(Details) = if(
      condition=MaxFileSize > 0 AND Details.Stat.Size > MaxFileSize,
      then= Details + DoNotUpload,
      else=if(
       // What to do about binaries? If they have an issuer name then
       // they are signed.
       condition=Details.Signatures.IssuerName,
       then=Details + ShouldUploadSignedBinary,
       else=Details + ShouldUploadAnyFile))

    // If the file is a binary, also add authenticode information.
    LET MaybeBinary(OSPath, Details) = ShouldUpload(Details=if(
       condition=Details.Magic =~ "PE.+executable",
       then=Details + dict(Signatures=authenticode(filename=OSPath)),
       else=Details))

    // Calculate the details column with hashes and magic.
    LET GetDetails(OSPath, Accessor) = MaybeBinary(
       OSPath=OSPath,
       Details=dict(Hashes=hash(path=OSPath, accessor=Accessor),
                    Stat=stat(filename=OSPath, accessor=Accessor),
                    Magic=magic(path=OSPath, accessor=Accessor)))

    {{ range $idx, $r := .Rules -}}
    {{- if $r.VQL -}}
    // {{ .Description }}
    LET Collect{{ $r.Target }}_{{ $r.Name }} =
{{ Indent .VQL 7 -}}
    {{- end -}}
    {{- end }}

    {{- range .TargetFiles -}}
    {{- if .Preamble }}
    // From {{ .Name }}
{{ Indent .Preamble 4 -}}
    {{- end -}}
    {{- end }}

sources:
- name: SearchGlobs
  notebook:
    - type: none
  query: |
    LET AllCollections <= Collections(Targets=scope())

    SELECT * FROM AllCollections
    --WHERE Glob

- name: All Matches Metadata
  notebook:
    - type: vql
      template: |
       // This cell generates other cells to preview the collected
       // data.  DO NOT recalculate this cell - each time new cells
       // will be added. Instead delete the notebook and allow
       // Velociraptor to recreate the entire notebook.
       LET ArtifactsWithResults <=
         SELECT pathspec(accessor="fs", parse=Data.VFSPath)[4] AS Artifact ,
           pathspec(accessor="fs", parse=Data.VFSPath)[-1][:-5] AS Source ,
           stat(accessor="fs", filename=Data.VFSPath + ".index").Size / 8 AS Records
         FROM enumerate_flow(client_id=ClientId, flow_id=FlowId)
         WHERE Type =~ "Result" AND Records > 0

       LET _ <= SELECT notebook_update_cell(notebook_id=NotebookId, type="vql",
       input=format(format='''
       /*
       # Results From %v
       */
       SELECT * FROM source(source=%q)
       ''', args=[Source, Source]),
       output=format(format='''
       <i>Recalculate</i> to show Results from <b>%v</b> with <b>%v</b> rows
       ''', args=[Source, Records])) AS NotebookModification
       FROM ArtifactsWithResults

       /*
       # Results Overview
       */
       SELECT Source, Records FROM ArtifactsWithResults ORDER BY Source


  query: |
    LET GlobLookup <= memoize(query=AllCollections, key="Glob")
    LET NTFSGlobs = SELECT * FROM AllCollections
       WHERE Glob AND Glob =~ "[:$]" AND NOT Glob =~ "\\$Recycle.Bin"
    LET AutoGlobs = SELECT * FROM AllCollections
       WHERE Glob AND ( Glob =~ "\\$Recycle.Bin" OR NOT Glob =~ "[:$]" )

    LET _ <= if(condition=MaxFileSize > 0,
                then=log(level="INFO", message="Limiting file acquisition to MaxFileSize %v bytes (%v)",
                         args=[MaxFileSize, humanize(bytes=MaxFileSize)]))

    LET PreferredAccessor <= if(
       condition=VSS_MAX_AGE_DAYS > 0,
       then="ntfs_vss", else="auto")

    LET AllGlobs = SELECT *
    FROM foreach(row={
        SELECT _value AS Device FROM foreach(row=Devices)
    }, query={
      SELECT * FROM chain(
      GlobNTFS={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               "ntfs" AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=NTFSGlobs.Glob, accessor="ntfs", root=Device)
      },
      GlobAuto={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               PreferredAccessor AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=AutoGlobs.Glob,
                  accessor=PreferredAccessor,
                  root=Device)
      })
    })
    WHERE NOT IsDir

    LET AllResults <= SELECT Type,
                             OSPath AS SourceFile,
                             Size,
                             Btime AS Created,
                             Ctime AS Changed,
                             Mtime AS Modified,
                             Atime AS LastAccessed,
                             Accessor,
                             Data
    FROM chain(
      {{ range $idx, $r := .Rules -}}
      {{- if $r.VQL -}}
      {{ $r.Target }}_{{ $r.Name }}={
        SELECT * FROM if(condition={
          SELECT * FROM AllCollections
          WHERE Rule = "{{ $r.Target }}/{{ $r.Name }}"
           AND log(message="Collecting Custom VQL Rule {{ $r.Target }}/{{ $r.Name }}",
                   dedup= -1 )
        }, then={
          SELECT "{{ $r.Target }}/{{ $r.Name }}" AS Type,
                 "{{ $r.Target }}/{{ $r.Name }}" AS Rule,
                 *
          FROM Collect{{ $r.Target }}_{{ $r.Name }}
        })
      },
      {{- end -}}
      {{- end }}
      Globs=AllGlobs
    )
    WHERE log(level="INFO", message="Found %v for rule %v", args=[
              SourceFile, Rule], dedup=10)

    SELECT * FROM AllResults
{{ range $idx, $r := .Rules -}}
{{- if $r.VQL }}
- name: {{ $r.Target }}_{{ $r.Name }}
  notebook:
    - type: none
  query: |
    SELECT * FROM foreach(row={
      SELECT dict(SourceFile=SourceFile, Size=Size, Modified=Modified) + Data AS Data
      FROM AllResults
      WHERE Type = "{{ $r.Target }}/{{ $r.Name }}"
    }, column="Data")
{{- end -}}
{{- end }}

- name: Uploads
  query: |
    // Initialize libmagic before we call it from multiple threads.
    LET _ <= magic(path="", accessor="data")

    -- Upload the files. Split into workers so the files are uploaded
    -- in parallel.
    LET uploaded_files = SELECT *
    FROM foreach(row={
       SELECT *
       FROM AllResults
       WHERE Size > 0
       },
          workers=30,

          // Do the heavy lifting in a thread
          query={
            SELECT * FROM foreach(row={
              SELECT GetDetails(OSPath=SourceFile,
                                  Accessor=Accessor) AS Details
              FROM scope()
            }, query={
              SELECT timestamp(epoch=now()) AS CopiedOnTimestamp,
                     Created,
                     Changed,
                     LastAccessed,
                     Modified,
                     SourceFile,
                     Size,
                     Details,
                     if(condition=Details.ShouldUpload,
                        then=upload(file=SourceFile,
                                    accessor=Accessor,
                                    mtime=Modified)) AS Upload
              FROM scope()
          })
      })

    -- Separate the hashes into their own column.
    SELECT CopiedOnTimestamp,
           SourceFile,
           Upload.Path AS DestinationFile,
           Size AS FileSize,
           Details.Hash.SHA256 AS SourceFileSha256,
           Created,
           Changed,
           Modified,
           LastAccessed,
           Details,
           Upload
    FROM uploaded_files

  notebook:
    - type: none
    - type: vql_suggestion
      name: Post process collection
      template: |
        /*

        # Post process this collection.

        Uncomment the following and evaluate the cell to create new
        collections based on the files collected from this artifact.

        The below VQL will apply remapping so standard artifacts will
        see the KapeFiles.Targets collection below as a virtual
        Windows Client. The artifacts will be collected to a temporary
        container and then re-imported as new collections into this
        client.

        NOTE: This is only a stop gap in case the proper artifacts
        were not collected in the first place. Parsing artifacts
        through a remapped collection is not as accurate as parsing
        directly on the endpoint. See
        https://docs.velociraptor.app/training/playbooks/preservation/
        for more info.

        */
        LET _ <= import(artifact="Windows.KapeFiles.Remapping")

        LET tmp <= tempfile()

        LET Results = SELECT import_collection(filename=Container, client_id=ClientId) AS Import
        FROM collect(artifacts=[
                       "Windows.Forensics.Usn",
                       "Windows.NTFS.MFT",
                     ],
                     args=dict(`Windows.Forensics.Usn`=dict(),
                               `Windows.NTFS.MFT`=dict()),
                     output=tmp,
                     remapping=GetRemapping(FlowId=FlowId, ClientId=ClientId))

        // SELECT * FROM Results

column_types:
- name: CopiedOnTimestamp
  type: timestamp
