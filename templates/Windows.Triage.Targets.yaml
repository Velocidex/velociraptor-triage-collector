{{ define "GlobTable" }}
  Target,Rule,Glob,Ref
  {{- range .Rules }}
  {{ .Target}},{{ .Name }},"{{ .Glob }}",{{ .Ref -}}
  {{ end }}
{{ end -}}

name: {{ .Config.Name }}
description: |
  This artifact aims to preserve raw files from the endpoint by
  selectively acquiring the most important files for the investigation
  at hand. The artfiact collates multiple rules from sources such as
  the KapeFiles repository.

  Kape is a popular bulk collector tool for triaging a system
  quickly. While KAPE itself is not an opensource tool, the logic it
  uses to decide which files to collect is encoded in YAML files
  hosted on the KapeFiles project
  (https://github.com/EricZimmerman/KapeFiles) and released under an
  MIT license.

  This artifact is automatically generated from these YAML files,
  contributed and maintained by the community. This artifact only
  encapsulates the KAPE "Targets" - basically a bunch of glob
  expressions used for collecting files on the endpoint. We do not
  do any post processing of these files - we just collect them.

  We recommend that timeouts and upload limits be used
  conservatively with this artifact because we can upload really
  vast quantities of data very quickly.

  NOTE:
    This artifact was built from [The Velociraptor Triage
    Repository](https://triage.velocidex.com/docs/)

  Commit {{ .Commit }} on {{ .Time }}

reference:
  - https://www.kroll.com/en/insights/publications/cyber/kroll-artifact-parser-extractor-kape
  - https://github.com/EricZimmerman/KapeFiles

parameters:
  - name: Devices
    type: json_array
    description: |
      Name of the drive letter to search. You can add multiple drives
      separated with a comma.
    default: '["C:"]'

  - name: DropVerySlowRules
    type: bool
    default: Y
    description: |
      Some rules are very slow due to a recursive search at the higher
      levels (For example a glob such as `C:\**\*.ini` ). These rules
      cause the collection to be very slow as the entire filesystem
      must be searched.

      By default we drop these rules but you can enable them if you
      like. This will cause the collection to be a lot slower.

  - name: VSS_MAX_AGE_DAYS
    type: int
    default: 0
    description: |
      If larger than zero we analyze VSS within this many days
      ago. (e.g 7 will analyze all VSS within the last week).  Note
      that when using VSS analysis we have to use the ntfs accessor
      for everything which will be much slower.

  - name: MaxFileSize
    type: int
    default: 18446744073709551615
    description: |
      The max size in bytes of the individual files to collect.
      Set to 0 to disable it.

  - name: UPLOAD_IS_RESUMABLE
    type: bool
    default: Y
    description: |
      If set the uploads can be resumed if the flow times out or
      errors.
{{ range .TargetFiles }}
  - name: {{ .Name }}
    description: "{{ .Description }}"
    type: bool
{{ end }}

export: |
  LET VQL_MATERIALIZE_ROW_LIMIT <= 10000
  LET NTFS_CACHE_TIME <= 100000
  LET NTFS_DISABLE_FULL_PATH_RESOLUTION <= TRUE
  LET S = scope()

  {{- if .Config.Debug }}
  LET GlobTable <= '''{{ template "GlobTable" . }}
  '''
  {{ else }}
  LET GlobTable <= gunzip(string=base64decode(string="{{ Compress "GlobTable" . }}"))
  {{ end }}

  LET SlowGlobRegex <= if(condition=S.DropVerySlowRules,
     then="^\\*\\*", else="RunSlowFileGlobs!!!")

  -- Group the targets for faster searching.
  LET TargetTable <= SELECT Target,
       enumerate(items=dict(Rule=Rule, Glob=Glob, Ref=Ref)) AS Rules
    FROM parse_csv(accessor="data",
  filename=GlobTable)
  GROUP BY Target

  //  Build a lookup cache on target.
  LET Lookup <= memoize(query={
    SELECT * FROM TargetTable
  }, key="Target")

  -- Extract all rules within the required target. Uses the memoized
  -- structure above.
  LET FilterTable(Required) =
     SELECT Required AS Target, *
     FROM flatten(query={
       SELECT * FROM foreach(row=get(item=Lookup, field=Required).Rules)
     })
     WHERE if(condition=Glob =~ SlowGlobRegex,
              then=log(level="INFO", message="Dropping rule %v/%v because it is too slow: %v",
                       dedup=-1, args=[Target, Rule, Glob]) AND FALSE,
              else=TRUE)

  LET Expand(FilteredTable) = SELECT * FROM foreach(
  row=FilteredTable,
  query={
    -- If there is a reference, resolve it from the table recursively.
    SELECT *
    FROM if(condition=Ref AND log(level="DEBUG", message="%v/%v: Resolving Ref %v",
         dedup=-1, args=[Target, Rule, Ref]),
    then={
       SELECT * FROM Expand(
          FilteredTable={
             SELECT * FROM FilterTable(Required=Ref)
          })
    }, else={
       SELECT Target, Rule, Glob FROM scope()
       WHERE if(condition=Glob,
         then=log(level="DEBUG", message="%v/%v: Glob is %v",
                  args=[Target, Rule, Glob], dedup=-1),
         else=TRUE)
    })
  })

  -- Collect all the top level targets that the user selected.
  LET Collections(Targets) = SELECT Target + "/" + Rule AS Rule, Glob
    FROM Expand(FilteredTable={
      SELECT Target,
            Rules.Rule AS Rule,
            Rules.Glob AS Glob,
            Rules.Ref AS Ref
     FROM flatten(query={
       SELECT * FROM TargetTable
       WHERE get(item=Targets, field=Target)
        AND log(level="INFO", message="Collecting target %v, Rules: %v",
             args=[Target, Rules.Rule], dedup=-1)
     })
    })
    GROUP BY Rule, Glob

    {{ range $idx, $r := .Rules -}}
    {{- if $r.VQL -}}
    // {{ .Description }}
    LET Collect{{ $r.Target }}_{{ $r.Name }} =
{{ Indent .VQL 7 -}}
    {{- end -}}
    {{- end }}

sources:
- name: SearchGlobs
  notebook:
    - type: none
  query: |
    LET AllCollections <= Collections(Targets=scope())

    SELECT * FROM AllCollections
    WHERE Glob

- name: All Matches Metadata
  notebook:
    - type: vql
      template: |
       // This cell generates other cells to preview the collected
       // data.  DO NOT recalculate this cell - each time new cells
       // will be added. Instead delete the notebook and allow
       // Velociraptor to recreate the entire notebook.
       LET ArtifactsWithResults <=
         SELECT pathspec(accessor="fs", parse=Data.VFSPath)[4] AS Artifact ,
           pathspec(accessor="fs", parse=Data.VFSPath)[-1][:-5] AS Source ,
           stat(accessor="fs", filename=Data.VFSPath + ".index").Size / 8 AS Records
         FROM enumerate_flow(client_id=ClientId, flow_id=FlowId)
         WHERE Type =~ "Result" AND Records > 0

       LET _ <= SELECT notebook_update_cell(notebook_id=NotebookId, type="vql",
       input=format(format='''
       /*
       # Results From %v
       */
       SELECT * FROM source(source=%q)
       ''', args=[Source, Source]),
       output=format(format='''
       <i>Recalculate</i> to show Results from <b>%v</b> with <b>%v</b> rows
       ''', args=[Source, Records])) AS NotebookModification
       FROM ArtifactsWithResults

       /*
       # Results Overview
       */
       SELECT Source, Records FROM ArtifactsWithResults ORDER BY Source


  query: |
    LET GlobLookup <= memoize(query=AllCollections, key="Glob")
    LET NTFSGlobs = SELECT * FROM AllCollections
       WHERE Glob AND Glob =~ "[:$]" AND NOT Glob =~ "\\$Recycle.Bin"
    LET AutoGlobs = SELECT * FROM AllCollections
       WHERE Glob AND ( Glob =~ "\\$Recycle.Bin" OR NOT Glob =~ "[:$]" )

    LET _ <= if(condition=MaxFileSize > 0 AND MaxFileSize < 100000000000,
                then=log(level="INFO", message="Limiting file acquisition to MaxFileSize %v bytes (%v)",
                         args=[MaxFileSize, humanize(bytes=MaxFileSize)]))

    LET PreferredAccessor <= if(
       condition=VSS_MAX_AGE_DAYS > 0,
       then="ntfs_vss", else="auto")

    LET AllResults <= SELECT Type,
                             OSPath AS SourceFile,
                             Size,
                             Btime AS Created,
                             Ctime AS Changed,
                             Mtime AS Modified,
                             Atime AS LastAccessed,
                             Accessor,
                             Data
    FROM foreach(row={
        SELECT _value AS Device FROM foreach(row=Devices)
    }, query={
      SELECT * FROM chain(
      {{ range $idx, $r := .Rules -}}
      {{- if $r.VQL -}}
      {{ $r.Target }}_{{ $r.Name }}={
        SELECT * FROM if(condition={
          SELECT *, Rule AS Type FROM AllCollections
          WHERE Rule = "{{ $r.Target }}/{{ $r.Name }}"
        }, then={
          SELECT "{{ $r.Target }}/{{ $r.Name }}" AS Type, *
          FROM Collect{{ $r.Target }}_{{ $r.Name }}
        })
      },
      {{- end -}}
      {{- end }}
      GlobNTFS={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               "ntfs" AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=NTFSGlobs.Glob, accessor="ntfs", root=Device)
      },
      GlobAuto={
        SELECT *,
               get(item=GlobLookup, field=Globs[0]).Rule AS Rule,
               PreferredAccessor AS Accessor,
               dict(Globs=Globs) AS Data,
               "Glob" AS Type
        FROM glob(globs=AutoGlobs.Glob,
                  accessor=PreferredAccessor,
                  root=Device)
      })
    })
    WHERE NOT IsDir
    AND log(level="INFO", message="Found %v for rule %v", args=[SourceFile, Rule], dedup=10)
    AND if(condition= Size <= MaxFileSize,
           then=TRUE,
           else=log(level="WARN", message="Skipping file %v (Size %v) Due to MaxFileSize",
                    dedup=-1, args=[SourceFile, humanize(bytes=Size)]) AND FALSE)

    SELECT * FROM AllResults
{{ range $idx, $r := .Rules -}}
{{- if $r.VQL }}
- name: {{ $r.Target }}_{{ $r.Name }}
  notebook:
    - type: none
  query: |
    SELECT * FROM foreach(row={
      SELECT dict(SourceFile=SourceFile, Size=Size, Modified=Modified) + Data AS Data
      FROM AllResults
      WHERE Type = "{{ $r.Target }}/{{ $r.Name }}"
    }, column="Data")
{{- end -}}
{{- end }}

- name: Uploads
  query: |
    -- Upload the files. Split into workers so the files are uploaded in parallel.
    LET uploaded_files = SELECT *
    FROM foreach(row={
       SELECT * FROM AllResults
       },
          workers=30,
          query={
            SELECT now() AS CopiedOnTimestamp,
                   Created,
                   Changed,
                   LastAccessed,
                   Modified,
                   SourceFile,
                   Size,
                   upload(file=SourceFile, accessor=Accessor, mtime=Modified) AS Upload
            FROM scope()
      })

    -- Separate the hashes into their own column.
    SELECT CopiedOnTimestamp,
           SourceFile,
           Upload.Path AS DestinationFile,
           Size AS FileSize,
           Upload.sha256 AS SourceFileSha256,
           Created,
           Changed,
           Modified,
           LastAccessed
    FROM uploaded_files

  notebook:
    - type: none
    - type: vql_suggestion
      name: Post process collection
      template: |
        /*

        # Post process this collection.

        Uncomment the following and evaluate the cell to create new
        collections based on the files collected from this artifact.

        The below VQL will apply remapping so standard artifacts will
        see the KapeFiles.Targets collection below as a virtual
        Windows Client. The artifacts will be collected to a temporary
        container and then re-imported as new collections into this
        client.

        NOTE: This is only a stop gap in case the proper artifacts
        were not collected in the first place. Parsing artifacts
        through a remapped collection is not as accurate as parsing
        directly on the endpoint. See
        https://docs.velociraptor.app/training/playbooks/preservation/
        for more info.

        */
        LET _ <= import(artifact="Windows.KapeFiles.Remapping")

        LET tmp <= tempfile()

        LET Results = SELECT import_collection(filename=Container, client_id=ClientId) AS Import
        FROM collect(artifacts=[
                       "Windows.Forensics.Usn",
                       "Windows.NTFS.MFT",
                     ],
                     args=dict(`Windows.Forensics.Usn`=dict(),
                               `Windows.NTFS.MFT`=dict()),
                     output=tmp,
                     remapping=GetRemapping(FlowId=FlowId, ClientId=ClientId))

        // SELECT * FROM Results
